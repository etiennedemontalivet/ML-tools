{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T11:43:14.095460Z",
     "start_time": "2020-05-29T11:43:14.090475Z"
    }
   },
   "source": [
    "This notebook provides a tools to **visualize the neural network prediction map during the backpropagation**. It aims at providing a visual tools to change the backpropagation hyperparameters and to see the impact on the model convergence. \n",
    "\n",
    "Please for any suggestion or comment: *etienne.demontalivet(at)gmail.com*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to use:\n",
    "\n",
    "Some of the hyperparameters are set in the *hp* dictionnary. After executing a few cells to initiate the callbacks and the neural network, you'll be able to execute the main cell to visualize the backpropagation effect on building the limits of the NN. After updating the *hp* parameters, just execute the main cell to see the impact of the update. \n",
    "\n",
    "At the end of the notebook you'll find some interesting set of hyperparameters.\n",
    "\n",
    "Enjoy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T13:42:29.292010Z",
     "start_time": "2020-06-02T13:42:29.288020Z"
    }
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hp = {\n",
    "    'lr':0.1,            # learning rate\n",
    "    'decay':0,           # decay \n",
    "    'momentum':0.9,      # mementum\n",
    "    'patience':50,       # number of epochs to wait before reducing the learning rate (see callback)\n",
    "    'epochs':100,        # number of epochs for the training\n",
    "    'n1':10,             # number of neurons in the first hidden layer\n",
    "    'n2':10,             # number of neurons in the second hidden layer\n",
    "    'n3':0,              # number of neurons in the third hidden layer\n",
    "    'kr':0,              # kernel regularization\n",
    "    'activation':'lrelu',# Type of activation to use in the hidden layers\n",
    "    'dataset':1          # dataset to use (see below)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T13:42:29.864389Z",
     "start_time": "2020-06-02T13:42:29.584735Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 datasets:\n",
    "- 1: Moons\n",
    "- 2: Circles\n",
    "- 3: Blobs\n",
    "- 4: tricky one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T13:42:30.582637Z",
     "start_time": "2020-06-02T13:42:30.215619Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate fake dataset:\n",
    "from sklearn.datasets import make_moons, make_circles, make_blobs, make_classification\n",
    "\n",
    "def get_dataset(case: int):\n",
    "    if case == 1:\n",
    "        X, y = make_moons(noise=0.1, random_state=42)\n",
    "    elif case == 2:\n",
    "        X, y = make_circles(noise=0.3, factor=0.5, random_state=40)\n",
    "    elif case == 3:\n",
    "        X, y = make_blobs(centers=2, n_features=2, random_state=42)\n",
    "    else:\n",
    "        X1, y1 = make_classification(n_classes=2, n_features=2, n_redundant=0, n_informative=2,\n",
    "                                   random_state=1, n_clusters_per_class=2)\n",
    "        X2, y2 = make_classification(n_classes=2, n_features=2, n_redundant=0, n_informative=2,\n",
    "                                   random_state=2, n_clusters_per_class=2)\n",
    "        X = np.concatenate([X1, X2])\n",
    "        y = np.concatenate([y1, y2])\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a Dense Neural Network:\n",
    "- 2 inputs (2 dimensions for visualization purpose)\n",
    "- 3 hidden layers (number of neurons per layer can be changed)\n",
    "- 1 output: we only use 1 output with a sigmoid activation as we only have 2 classes\n",
    "\n",
    "We use the common *binary-crossentropy* / *sigmoÃ¯d* pair for the output. We remove the decay applied by default to the SGD, so we can see the *learning rate* being decreased only by our other callback (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T13:42:34.980696Z",
     "start_time": "2020-06-02T13:42:32.680736Z"
    }
   },
   "outputs": [],
   "source": [
    "# MODEL\n",
    "# We use a Dense Neural Net with 2 inputs, 2 hidden layers and one output\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LeakyReLU, ReLU, Activation\n",
    "from keras import regularizers\n",
    "\n",
    "N_HIDDEN = 3\n",
    "\n",
    "def create_model(l1_neurons: int, l2_neurons: int, l3_neurons: int, kernel_reg: float, activation: str='relu'):\n",
    "    # Get model parameters\n",
    "    kr = regularizers.l2(kernel_reg)\n",
    "    if activation == 'relu':\n",
    "        act_layer = [ Activation('relu') for i in range(N_HIDDEN) ]\n",
    "    elif activation == 'lrelu':\n",
    "        act_layer = [ keras.layers.LeakyReLU(alpha=0.3) for i in range(N_HIDDEN) ]\n",
    "    elif activation == 'sigmoid':\n",
    "        act_layer = [ Activation('sigmoid') for i in range(N_HIDDEN) ]\n",
    "    elif activation == 'tanh':\n",
    "        act_layer = [ Activation('tanh') for i in range(N_HIDDEN) ]\n",
    "    else:\n",
    "        raise ValueError(str(activation) + \" is not yet supported. Please implement it or choose between ['relu', 'lrelu', 'sigmoid']\")\n",
    "    \n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(l1_neurons, input_dim=2, kernel_regularizer=kr))\n",
    "    model.add(act_layer[0])\n",
    "    if l2_neurons > 0:\n",
    "        model.add(Dense(l2_neurons, kernel_regularizer=kr))\n",
    "        model.add(act_layer[1])\n",
    "    if l3_neurons > 0:\n",
    "        model.add(Dense(l3_neurons, kernel_regularizer=kr))\n",
    "        model.add(act_layer[2])\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Optimizer\n",
    "    optim = keras.optimizers.SGD(lr=hp['lr'], decay=hp['decay'], momentum=hp['momentum'], nesterov=False)\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The callback below evaluates each meshgrid point probability and associate a color depending on the model prediction. It is called at the end of each epoch, so we can visualize the convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:16:08.129924Z",
     "start_time": "2020-06-03T16:16:08.116926Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "import time\n",
    "\n",
    "class PlotCallback(keras.callbacks.Callback):\n",
    "    '''\n",
    "    Keras callback to draw the prediction map at the end of each epoch\n",
    "    '''\n",
    "    def __init__(self, total_epochs: int):\n",
    "        self.tot_epochs = total_epochs\n",
    "        self.contours = None\n",
    "        self.text = None\n",
    "        self.lr = K.eval(model.optimizer.lr)\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.nb_epochs = 0\n",
    "        \n",
    "        # Get the proba of each mesh grid point\n",
    "        Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])  # [:, 1]\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "\n",
    "        self.contours = ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)        \n",
    "\n",
    "        # Evaluate the current score\n",
    "        score = model.evaluate(X_test, y_test)\n",
    "\n",
    "        # Update text on figure\n",
    "        if self.text is not None:\n",
    "            self.text.remove()\n",
    "        self.text = ax.text(xx.max() - 0.1, yy.min() - 0.5, ('epochs: %d/%d - loss:%.2f - acc:%.2f - lr:%.2e' % (self.nb_epochs, self.tot_epochs, score[0], score[1], K.eval(model.optimizer.lr))).lstrip('0'),\n",
    "                size=12, horizontalalignment='right')\n",
    "        \n",
    "        fig.canvas.draw()\n",
    "        # Wait for keyboard input before starting fitting\n",
    "        input()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.nb_epochs = self.nb_epochs + 1\n",
    "\n",
    "        # Get the proba of each mesh grid point\n",
    "        Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])  # [:, 1]\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "\n",
    "        # Remove the old contour map\n",
    "        for co in self.contours.collections:\n",
    "            co.remove()\n",
    "        self.contours = ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)        \n",
    "\n",
    "        # Evaluate the current score\n",
    "        score = model.evaluate(X_test, y_test)\n",
    "\n",
    "        # Update text on figure\n",
    "        if self.text is not None:\n",
    "            self.text.remove()\n",
    "        self.text = ax.text(xx.max() - 0.1, yy.min() - 0.5, ('epochs: %d/%d - loss:%.2f - acc:%.2f - lr:%.2e' % (self.nb_epochs, self.tot_epochs, score[0], score[1], K.eval(model.optimizer.lr))).lstrip('0'),\n",
    "                size=12, horizontalalignment='right')\n",
    "        \n",
    "        fig.canvas.draw()\n",
    "        \n",
    "        if self.nb_epochs == hp['epochs']:\n",
    "            time.sleep(2) # for video recording purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following callback reduces by *factor* the *learning rate* when the loss does not improve (see *min_delta*) during *patience* epochs. It although prevent from overfitting (try to remove it ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T13:42:38.181164Z",
     "start_time": "2020-06-02T13:42:38.177131Z"
    }
   },
   "outputs": [],
   "source": [
    "# CALLBACKS\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "def get_callbacks(n_epochs: int):\n",
    "    reduceOnPlateau = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=hp['patience'],\n",
    "        verbose=0,\n",
    "        mode='auto',\n",
    "        min_delta=0.0001,\n",
    "        cooldown=0,\n",
    "        min_lr=4e-5\n",
    "    )\n",
    "    \n",
    "    return [PlotCallback(n_epochs), reduceOnPlateau ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T13:42:40.126482Z",
     "start_time": "2020-06-02T13:42:40.039714Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes the **main cell**. There are a lot of settings... Note the few last lines which contains other NN fitting parameters (epochs number, batch size,...). Change the hyperparameters and look at the impact on the convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T08:23:49.196846Z",
     "start_time": "2020-06-04T08:23:36.976176Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "h = .2  # step size in the mesh. Increase it if your CPU load is too high\n",
    "\n",
    "X, y = get_dataset(hp['dataset'])\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_axes([0.05, 0.1, 0.7, 0.8])\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=.4, random_state=42)\n",
    "\n",
    "# Get the mesh coordinates\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# Color(map) settings\n",
    "cm = plt.cm.RdBu\n",
    "cm_r = '#FF0000'\n",
    "cm_b = '#0000FF'\n",
    "cm_bright = ListedColormap([cm_r, cm_b])\n",
    "\n",
    "# Plot the training points\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "           edgecolors='k', zorder=2.5)\n",
    "# Plot the testing points\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n",
    "           edgecolors='k', marker=\"s\", zorder=2.5)\n",
    "\n",
    "# Set axis limits\n",
    "ax.set_xlim(xx.min(), xx.max())\n",
    "ax.set_ylim(yy.min(), yy.max())\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks(())\n",
    "\n",
    "# Legend\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='black', markerfacecolor=cm_b, label='class 1 X_train'),\n",
    "                   Line2D([0], [0], marker='s', color='black', markerfacecolor=cm_b, label='class 1 X_test'),\n",
    "                   Line2D([0], [0], marker='o', color='black', markerfacecolor=cm_r, label='class 2 X_train'),\n",
    "                   Line2D([0], [0], marker='s', color='black', markerfacecolor=cm_r, label='class 2 X_test')]\n",
    "ax.legend(handles=legend_elements, bbox_to_anchor=(1.28, 0.95))\n",
    "\n",
    "# Title\n",
    "ax.set(title='Neural Network prediction map')\n",
    "norm = mpl.colors.Normalize(vmin=0, vmax=100)\n",
    "\n",
    "# Personal text\n",
    "ax.text(xx.min()+0.01*(xx.max()-xx.min()), yy.min()+0.01*(yy.max()-yy.min()),('github.com/etiennedemontalivet/ML-tools'))\n",
    "\n",
    "# Colorbars\n",
    "axins = inset_axes(ax,\n",
    "    width=\"2%\",  # width = 5% of parent_bbox width\n",
    "    height=\"50%\",  # height : 50%\n",
    "    loc='lower left',\n",
    "    bbox_to_anchor=(1.2, 0.12, 1, 1),\n",
    "    bbox_transform=ax.transAxes,\n",
    "    borderpad=0,\n",
    ")\n",
    "axins2 = inset_axes(ax,\n",
    "    width=\"2%\",  # width = 5% of parent_bbox width\n",
    "    height=\"50%\",  # height : 50%\n",
    "    loc='lower left',\n",
    "    bbox_to_anchor=(1.04, 0.12, 1, 1),\n",
    "    bbox_transform=ax.transAxes,\n",
    "    borderpad=0,\n",
    ")\n",
    "plt.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=plt.cm.Blues), cax=axins2, orientation='vertical', label='% prediction class 1')\n",
    "plt.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=plt.cm.Reds), cax=axins, orientation='vertical', label='% prediction class 2')\n",
    "\n",
    "# Show figure\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "\n",
    "# create and fit the model\n",
    "model = create_model(l1_neurons=hp['n1'], l2_neurons=hp['n2'], l3_neurons=hp['n3'], kernel_reg=hp['kr'], activation=hp['activation'])\n",
    "n_epochs = hp['epochs']\n",
    "# model.load_weights('test')\n",
    "# model.save_weights('test')\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=10, epochs=n_epochs, verbose=0, shuffle=True, callbacks=get_callbacks(n_epochs=n_epochs))\n",
    "\n",
    "# Close figure afterwards\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T08:22:33.652967Z",
     "start_time": "2020-06-04T08:22:33.625572Z"
    }
   },
   "outputs": [],
   "source": [
    "# Change hyperparameters here and execute main cell again\n",
    "hp = {\n",
    "    'lr':0.07,           # learning rate\n",
    "    'decay':0.,          # decay \n",
    "    'momentum':0.99,     # momentum\n",
    "    'patience':50,       # number of epochs to wait before reducing the learning rate (see callback)\n",
    "    'epochs':400,        # number of epochs for the training\n",
    "    'n1':10,             # number of neurons in the first hidden layer\n",
    "    'n2':10,             # number of neurons in the second hidden layer\n",
    "    'n3':0,              # number of neurons in the third hidden layer\n",
    "    'kr':0,              # kernel regularization\n",
    "    'activation':'lrelu',# Type of activation to use in the hidden layers\n",
    "    'dataset':2          # dataset to use\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    "This notebook aims at being a tools to visualize the backpropagation and is not 100% correct mathematically speaking. Some shortcuts have been taken for simplification purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific set of parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few examples of hyperparameters with their effects. To visualize one, execute the corresponding cell and then execute the main cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make your gradient explode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we choose a too high learning rate, this will result in exploding the gradient and your network won't learn anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T13:47:16.006632Z",
     "start_time": "2020-06-02T13:47:16.000648Z"
    }
   },
   "outputs": [],
   "source": [
    "hp = {\n",
    "    'lr':0.5,            # learning rate\n",
    "    'decay':0,           # decay \n",
    "    'momentum':0.9,      # momentum\n",
    "    'patience':50,       # number of epochs to wait before reducing the learning rate (see callback)\n",
    "    'epochs':150,        # number of epochs for the training\n",
    "    'n1':10,             # number of neurons in the first hidden layer\n",
    "    'n2':10,             # number of neurons in the second hidden layer\n",
    "    'n3':0,              # number of neurons in the third hidden layer\n",
    "    'kr':0,              # kernel regularization\n",
    "    'activation':'lrelu',# Type of activation to use in the hidden layers\n",
    "    'dataset':1          # dataset to use\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare without and with momentum. All other parameters are fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T13:31:12.694526Z",
     "start_time": "2020-06-02T13:31:12.689539Z"
    }
   },
   "outputs": [],
   "source": [
    "# without\n",
    "hp = {\n",
    "    'lr':0.07,           # learning rate\n",
    "    'decay':0,           # decay \n",
    "    'momentum':0,        # WITHOUT momentum\n",
    "    'patience':50,       # number of epochs to wait before reducing the learning rate (see callback)\n",
    "    'epochs':150,        # number of epochs for the training\n",
    "    'n1':10,             # number of neurons in the first hidden layer\n",
    "    'n2':10,             # number of neurons in the second hidden layer\n",
    "    'n3':0,              # number of neurons in the third hidden layer\n",
    "    'kr':0,              # kernel regularization\n",
    "    'activation':'lrelu',# Type of activation to use in the hidden layers\n",
    "    'dataset':1          # dataset to use\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T13:30:17.494813Z",
     "start_time": "2020-06-02T13:30:17.490848Z"
    }
   },
   "outputs": [],
   "source": [
    "# with\n",
    "hp = {\n",
    "    'lr':0.07,           # learning rate\n",
    "    'decay':0,           # decay \n",
    "    'momentum':0.9,      # WITH momentum\n",
    "    'patience':50,       # number of epochs to wait before reducing the learning rate (see callback)\n",
    "    'epochs':150,        # number of epochs for the training\n",
    "    'n1':10,             # number of neurons in the first hidden layer\n",
    "    'n2':10,             # number of neurons in the second hidden layer\n",
    "    'n3':0,              # number of neurons in the third hidden layer\n",
    "    'kr':0,              # kernel regularization\n",
    "    'activation':'lrelu',# Type of activation to use in the hidden layers\n",
    "    'dataset':1          # dataset to use\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we want to achieve a good convergence without momentum, we have to increase the number of epochs and the patience (not to decrease the learning rate too quickly) !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T13:32:25.753683Z",
     "start_time": "2020-06-02T13:32:25.748696Z"
    }
   },
   "outputs": [],
   "source": [
    "# without\n",
    "hp = {\n",
    "    'lr':0.07,           # learning rate\n",
    "    'decay':0,           # decay \n",
    "    'momentum':0,        # WITHOUT momentum\n",
    "    'patience':500,       # number of epochs to wait before reducing the learning rate (see callback)\n",
    "    'epochs':500,        # number of epochs for the training\n",
    "    'n1':10,             # number of neurons in the first hidden layer\n",
    "    'n2':10,             # number of neurons in the second hidden layer\n",
    "    'n3':0,              # number of neurons in the third hidden layer\n",
    "    'kr':0,              # kernel regularization\n",
    "    'activation':'lrelu',# Type of activation to use in the hidden layers\n",
    "    'dataset':1          # dataset to use\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T08:39:49.028626Z",
     "start_time": "2020-06-01T08:39:49.024637Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hp = {\n",
    "    'lr':0.002,           # learning rate\n",
    "    'decay':0,           # decay \n",
    "    'momentum':0.999,    # momentum\n",
    "    'patience':50,       # number of epochs to wait before reducing the learning rate (see callback)\n",
    "    'epochs':500,        # number of epochs for the training\n",
    "    'n1':50,             # number of neurons in the first hidden layer\n",
    "    'n2':50,             # number of neurons in the second hidden layer\n",
    "    'n3':50,             # number of neurons in the third hidden layer\n",
    "    'kr':0,              # kernel regularization\n",
    "    'activation':'lrelu',# Type of activation to use in the hidden layers\n",
    "    'dataset':2          # dataset to use\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"NN_overfitting.gif\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "ipub": {
   "titlepage": {
    "author": "Etienne de Montalivet",
    "email": "etienne.demontalivet@gmail.com",
    "title": "Dimension Reduction: an introduction with PCA and LDA"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
